{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only reading in one email for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroncsv = \"../experiments/data/enron.csv\"\n",
    "metadataHeaders = '../experiments/data/metadataHeaders.csv'\n",
    "spam = '../experiments/data/spam.csv'\n",
    "spamSubjects = '../experiments/data/spamWords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = pd.read_csv(metadataHeaders, sep=',').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DONE --\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(enroncsv, names=columns, sep='|', low_memory=False)\n",
    "\n",
    "print(\"-- DONE --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DONE --\n"
     ]
    }
   ],
   "source": [
    "removableColumns = pd.read_csv('data/removableColumns.csv', sep=',').columns.tolist()\n",
    "\n",
    "df.drop(removableColumns, axis=1)\n",
    "\n",
    "print('-- DONE --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DONE --\n"
     ]
    }
   ],
   "source": [
    "listOfEmailsForBagOfWords = df.loc[df['Directory'].str.contains('inbox')]\n",
    "\n",
    "print('-- DONE --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498161"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsPerEmail = df[pd.notnull(df['Subject'])]\n",
    "nonSpamSubjects = wordsPerEmail['Subject']\n",
    "wordsPerEmail = nonSpamSubjects.str.split(' ')\n",
    "len(wordsPerEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "printable = set(string.printable)\n",
    "\n",
    "def isEnglish(s):\n",
    "    for x in s:\n",
    "        if x not in printable:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def addToBagOfWords(dictionary, arr):\n",
    "    for word in arr:\n",
    "        word = word.lower()\n",
    "        \n",
    "        if '=' in word:\n",
    "            continue\n",
    "        if '/' in word:\n",
    "            continue\n",
    "        if '\\\\' in word:\n",
    "            continue\n",
    "        if '_' in word:\n",
    "            continue\n",
    "        if '-' in word:\n",
    "            continue\n",
    "        if ':' in word:\n",
    "            continue\n",
    "        if '@' in word:\n",
    "            continue\n",
    "        if '#' in word:\n",
    "            continue\n",
    "        if '$' in word:\n",
    "            word = '$'\n",
    "\n",
    "        word = word.replace('.', '')\n",
    "        word = word.replace(')', '')\n",
    "        word = word.replace('(', '')\n",
    "        word = word.replace('&', '')\n",
    "        word = word.replace('\\'', '')\n",
    "        word = word.replace('\\\"', '')\n",
    "        word = word.replace(',', '')\n",
    "        word = word.replace('[', '')\n",
    "        word = word.replace(']', '')\n",
    "        word = word.replace('{', '')\n",
    "        word = word.replace('}', '')\n",
    "        word = word.replace(';', '')\n",
    "\n",
    "        if word == '':\n",
    "            continue\n",
    "\n",
    "        if word in dictionary:\n",
    "            dictionary[word] = dictionary[word] + 1\n",
    "        else:\n",
    "            dictionary[word] = 1\n",
    "            \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToDictionary(dictionary, arr):\n",
    "    for index, subject in arr.iteritems():\n",
    "        dictionary = addToBagOfWords(dictionary, subject)\n",
    "                \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39519"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = {}\n",
    "\n",
    "dictionary = addToDictionary(dictionary, wordsPerEmail)\n",
    "        \n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spamSubjects = pd.read_csv(spam, names=['Subject'], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45090"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsPerEmail = spamSubjects['Subject'].str.split(' ')\n",
    "\n",
    "dictionary = addToDictionary(dictionary, wordsPerEmail)\n",
    "        \n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45090"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionaryList = []\n",
    "for key in dictionary:\n",
    "    dictionaryList.append([key, dictionary[key]])\n",
    "    \n",
    "dictionaryList = pd.DataFrame(dictionaryList, columns=[\"Word\", \"Occurances\"])\n",
    "len(dictionaryList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use words that occurred more than 30 times in our bag of words as other words would almost never match up anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5751"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionaryList = dictionaryList.loc[dictionaryList['Occurances'] > 30].reset_index(drop=True)\n",
    "len(dictionaryList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBagToInputOutput(inputItems, outputItems, arr):\n",
    "    for index, subject in inputItems.iteritems():\n",
    "        posBagOfWords = {}\n",
    "        posBagOfWords = addToBagOfWords(posBagOfWords, subject)\n",
    "\n",
    "        listOfIn = []\n",
    "        for key in posBagOfWords:\n",
    "            listOfIn.append(key)\n",
    "\n",
    "        inputBag = dictionaryList['Word'].isin(listOfIn).values.tolist()\n",
    "\n",
    "        arr.append([np.array(inputBag), np.array(outputItems)])\n",
    "        \n",
    "    return arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "training = []\n",
    "\n",
    "NON_SPAM_SAMPLES = 250\n",
    "SPAM_SAMPLES = 250\n",
    "\n",
    "trainingPos = df[pd.notnull(df['Subject'])]\n",
    "trainingPos = trainingPos.sample(NON_SPAM_SAMPLES)\n",
    "trainingPos = trainingPos['Subject']\n",
    "\n",
    "trainingPosSplit = trainingPos.str.split(' ')\n",
    "\n",
    "training = addBagToInputOutput(trainingPosSplit, [0], training)\n",
    "    \n",
    "trainingNeg = spamSubjects.sample(SPAM_SAMPLES)\n",
    "trainingNeg = trainingNeg['Subject']\n",
    "\n",
    "trainingNegSplit = trainingNeg.str.split(' ')\n",
    "    \n",
    "training = addBagToInputOutput(trainingNegSplit, [1], training)\n",
    "\n",
    "training = np.array(training)\n",
    "np.random.shuffle(training)\n",
    "\n",
    "len(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up a neuarl network with neurons on each layer \n",
    "\n",
    "    input - 5558 + bias\n",
    "    hidden layer 1 - 200 + bias\n",
    "    hidden layer 2 - 20 + bias\n",
    "    output - 1\n",
    "    \n",
    "The learing rate is set to 0.001\n",
    "The netword is bound to (-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpy.numberGenerator.bounds import Bounds\n",
    "from mlpy.neuralNetwork.feedForwardNeuralNetwork import NeuralNetwork\n",
    "from mlpy.neuralNetwork.structure.layer import Layer\n",
    "\n",
    "l_rate = 0.001\n",
    "bounds = Bounds(-2, 2)\n",
    "\n",
    "inputLayer = Layer(bounds, size = len(training[0][0]), prev = None, l_rate = l_rate, bias = True, label = \"Input layer\")\n",
    "hiddenLayer = Layer(bounds, size = 200, prev = inputLayer, l_rate = l_rate, bias = True, label = \"Hidden layer\")\n",
    "hiddenLayer2 = Layer(bounds, size = 20, prev = hiddenLayer, l_rate = l_rate, bias = True, label = \"Hidden layer 2\")\n",
    "outputLayer = Layer(bounds, size = len(training[0][1]), prev = hiddenLayer2, l_rate = l_rate, bias = False, label = \"Output layer\")\n",
    "\n",
    "fnn = NeuralNetwork()\n",
    "fnn.appendLayer(inputLayer)\n",
    "fnn.appendLayer(hiddenLayer)\n",
    "fnn.appendLayer(hiddenLayer2)\n",
    "fnn.appendLayer(outputLayer)\n",
    "\n",
    "group_training = np.array([input[0] for input in training])\n",
    "group_target = np.array([output[1] for output in training])\n",
    "\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run training over 4000 iterations and output the mean error every 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting..\n",
      "0.0%\t -0.4984291087363783\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5396c2f889dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0min_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mlpy/neuralNetwork/feedForwardNeuralNetwork.py\u001b[0m in \u001b[0;36mbackPropagation\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyDeltaWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mlpy/neuralNetwork/structure/layer.py\u001b[0m in \u001b[0;36mapplyDeltaWeights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplyDeltaWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltaWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddBias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ITERATIONS = 4000\n",
    "\n",
    "print(\"Starting..\")\n",
    "for i in range(ITERATIONS):\n",
    "    mod = i % len(training)\n",
    "    in_out = training[mod]\n",
    "    result = fnn.fire(group_training)\n",
    "    error = fnn.backPropagation(group_target)\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(str(np.round(i/ITERATIONS*100)) + '%\\t', error.mean())\n",
    "    \n",
    "print(\"-- DONE --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = []\n",
    "\n",
    "TEST_NON_SPAM_SAMPLES = 250\n",
    "TEST_SPAM_SAMPLES = 250\n",
    "\n",
    "trainingPos = df[pd.notnull(df['Subject'])]\n",
    "trainingPos = trainingPos.sample(TEST_NON_SPAM_SAMPLES)\n",
    "trainingPos = trainingPos['Subject']\n",
    "\n",
    "trainingPosSplit = trainingPos.str.split(' ')\n",
    "\n",
    "testing = addBagToInputOutput(trainingPosSplit, [0], testing)\n",
    "    \n",
    "trainingNeg = spamSubjects.sample(TEST_SPAM_SAMPLES)\n",
    "trainingNeg = trainingNeg['Subject']\n",
    "\n",
    "trainingNegSplit = trainingNeg.str.split(' ')\n",
    "\n",
    "testing = addBagToInputOutput(trainingNegSplit, [1], testing)\n",
    "\n",
    "testing = np.array(testing)\n",
    "np.random.shuffle(testing)\n",
    "\n",
    "len(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model we will take the test data and use it as an measure of performace. The following will be output:\n",
    "\n",
    "1. The Classification Accuracy\n",
    "2. The Non-spam Classsification accuracy\n",
    "3. The Spam Classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy:  0.896\n",
      "Non spam classification accuracy:  0.892\n",
      "Spam classification accuracy:  0.9\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "spamCorrect = 0\n",
    "nonSpamCorrect = 0\n",
    "for i in range(len(testing)):\n",
    "    in_out = testing[i]\n",
    "    result = fnn.fire(np.array([in_out[0]]))\n",
    "    \n",
    "    target = in_out[1][0]\n",
    "    result = np.round(result[0][0])\n",
    "\n",
    "    if result == target:\n",
    "        correct += 1\n",
    "        if target == 1:\n",
    "            spamCorrect += 1\n",
    "        else:\n",
    "            nonSpamCorrect += 1\n",
    "        \n",
    "        \n",
    "print(\"Classification accuracy: \", correct / len(testing))\n",
    "print(\"Non spam classification accuracy: \", nonSpamCorrect / TEST_NON_SPAM_SAMPLES)\n",
    "print(\"Spam classification accuracy: \", spamCorrect / TEST_SPAM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
